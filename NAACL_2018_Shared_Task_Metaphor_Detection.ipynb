{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAACL 2018 Shared Task - Metaphor Detection\n",
    "\n",
    "Add description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "\n",
    "- Facebook FastText Embeddings for English\n",
    "- https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "\n",
    "## Preflight Checks\n",
    "\n",
    "- Installed requirements.txt\n",
    "- (optional) Download vuamc.zip http://ota.ahds.ac.uk/headers/2541.xml\n",
    "\n",
    "https://github.com/EducationalTestingService/metaphor/tree/master/NAACL-FLP-shared-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for dependencies\n",
    "import utils\n",
    "import corpus\n",
    "import evaluate\n",
    "import features\n",
    "import numpy\n",
    "\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, Input, Masking, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as kerasbackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for VUAMC CSV files and generate if necessary\n",
    "\n",
    "if not os.path.exists('source/vuamc_corpus_test.csv') and not os.path.exists('source/vuamc_corpus_train.csv'):\n",
    "    print('VUAMC training and test data not found. Generating...')\n",
    "    # utils.download_vuamc_xml()\n",
    "    # utils.generate_vuamc_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and validated training corpus\n",
      "Loaded and validated test corpus\n"
     ]
    }
   ],
   "source": [
    "# Load Train Corpus from CSV\n",
    "c_train = corpus.VUAMC('source/vuamc_corpus_train.csv', 'source/verb_tokens_train_gold_labels.csv')\n",
    "c_train.validate_corpus()\n",
    "print('Loaded and validated training corpus')\n",
    "\n",
    "# Load Test Corpus from CSV\n",
    "c_test = corpus.VUAMC('source/vuamc_corpus_test.csv', 'source/verb_tokens_test.csv', mode='test')\n",
    "c_test.validate_corpus()\n",
    "print('Loaded and validated test corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of metaphor tokens: 3\n",
      "Percentage of non-metaphor tokens: 97\n",
      "Ratio: 1:32\n"
     ]
    }
   ],
   "source": [
    "# Shows that we got imbalanced classes in the training data\n",
    "number_of_all_labels = len( c_train.label_list)\n",
    "count_of_label_classes = collections.Counter( c_train.label_list)\n",
    "\n",
    "percentage_of_non_metaphor_tokens = round(count_of_label_classes[0] / number_of_all_labels * 100)\n",
    "percentage_of_metaphor_tokens = round(count_of_label_classes[1] / number_of_all_labels * 100)\n",
    "ratio = utils.simplify_ratio(percentage_of_non_metaphor_tokens, percentage_of_metaphor_tokens)\n",
    "assert(percentage_of_non_metaphor_tokens + percentage_of_metaphor_tokens == 100)\n",
    "\n",
    "print('Percentage of metaphor tokens: {}'.format(percentage_of_metaphor_tokens))\n",
    "print('Percentage of non-metaphor tokens: {}'.format(percentage_of_non_metaphor_tokens))\n",
    "print('Ratio: {}:{}'.format(ratio[0], ratio[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "KERAS_OPTIMIZER = 'rmsprop'\n",
    "KERAS_METRICS = ['categorical_accuracy']\n",
    "KERAS_EPOCHS = 1\n",
    "KERAS_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted Word Embeddings\n"
     ]
    }
   ],
   "source": [
    "# embeddings = features.Word2Vec()\n",
    "embeddings = features.DummyEmbeddings(EMBEDDING_DIM)\n",
    "x, y = features.generate_input_and_labels(c_train.sentences, Vectors=embeddings)\n",
    "x_test, y_test = features.generate_input_and_labels(c_test.sentences, Vectors=embeddings)\n",
    "\n",
    "# Free up some memory\n",
    "del embeddings\n",
    "print('Deleted Word Embeddings')\n",
    "\n",
    "# Input data and categorical labels\n",
    "x_input = x\n",
    "y_labels = to_categorical(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train Data tensor: (6299, 50, 300)\n",
      "Shape of Train Labels tensor: (6299, 50, 2)\n",
      "Shape of Validation Data tensor: (1574, 50, 300)\n",
      "Shape of validation Labels tensor: (1574, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "# Generate Training and Validation split\n",
    "indices = numpy.arange(x_input.shape[0])\n",
    "numpy.random.shuffle(indices)\n",
    "data = x_input[indices]\n",
    "labels = y_labels[indices]\n",
    "num_validation_samples = int(0.2 * x_input.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Shape of Train Data tensor:', x_train.shape)\n",
    "print('Shape of Train Labels tensor:', y_train.shape)\n",
    "print('Shape of Validation Data tensor:', x_val.shape)\n",
    "print('Shape of validation Labels tensor:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_weights 1 : 32\n"
     ]
    }
   ],
   "source": [
    "# Generate loss_weight, since out dataset contains 97% non-metaphor tokens\n",
    "# TODO: calculate that shice\n",
    "loss_weight = 32\n",
    "# KERAS_LOSS = 'categorical_crossentropy'\n",
    "KERAS_LOSS = utils.weighted_categorical_crossentropy([1, loss_weight])\n",
    "print('loss_weights 1 : {}'.format(loss_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6299 samples, validate on 1574 samples\n",
      "Epoch 1/1\n",
      "6299/6299 [==============================] - 19s 3ms/step - loss: 0.7444 - categorical_accuracy: 0.7919 - val_loss: 0.5039 - val_categorical_accuracy: 0.8330\n",
      "1574/1574 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create and compile model\n",
    "inputs = Input(shape=(MAX_SENTENCE_LENGTH, EMBEDDING_DIM))\n",
    "model = Masking(mask_value=[-1] * EMBEDDING_DIM)(inputs)\n",
    "model = Bidirectional(LSTM(100, return_sequences=True, dropout=0, recurrent_dropout=0.25))(model)\n",
    "outputs = TimeDistributed(Dense(2, activation='softmax'))(model)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=KERAS_OPTIMIZER, loss=KERAS_LOSS, metrics=KERAS_METRICS)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, batch_size=KERAS_BATCH_SIZE, epochs=KERAS_EPOCHS, validation_data=(x_val, y_val))\n",
    "scores = model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(precision=0.6616130988477865, recall=0.6616130988477865, f1=0.6616130988477865)\n"
     ]
    }
   ],
   "source": [
    "# Generate list of label predictions for each sentence\n",
    "float_predictions = model.predict(x_test, batch_size=KERAS_BATCH_SIZE)\n",
    "binary_predictions = kerasbackend.argmax(float_predictions)\n",
    "label_predictions = kerasbackend.eval(binary_predictions)\n",
    "\n",
    "# Write prediction to CSV file\n",
    "predictions_file = 'predictions.csv'\n",
    "standard_file = 'source/verb_tokens_test_gold_labels.csv'\n",
    "\n",
    "rows = evaluate.corpus_evaluation(c_test, label_predictions, MAX_SENTENCE_LENGTH)\n",
    "evaluate.csv_evalutation(rows, predictions_file)\n",
    "results = evaluate.precision_recall_f1(predictions_file, standard_file)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
