{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAACL 2018 Shared Task - Metaphor Detection\n",
    "\n",
    "This notebook implements a method for Metaphor Detection using Keras, as part of my [bachelor thesis](TODO). It is based on the [NAACL 2018 Shared Task for Metaphor Detection](https://sites.google.com/site/figlangworkshop/shared-task) but did not compete in the task.\n",
    "\n",
    "For further details on the Shared Task dataset, visit: https://github.com/EducationalTestingService/metaphor/tree/master/NAACL-FLP-shared-task\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Prerequisites](#prerequisites)\n",
    "- [Download VUAM Corpus](#vuamc_generation)\n",
    "- [Generate Training and Test Data](#corpus_generation)\n",
    "- [Validate Training and Test Data](#corpus_validation)\n",
    "- [Keras Model Configuration](#model_configuration)\n",
    "- [Load Word Embeddings](#word_embeddings)\n",
    "- [Keras Model Compilation](#model_compilation)\n",
    "- [training](#training)\n",
    "- [evaluation](#evaluation)\n",
    "- [training_plot](#training_plot)\n",
    "\n",
    "<a id='prerequisites'></a>\n",
    "## Prerequisites \n",
    "\n",
    "Install the Python3 requirements from the requirements.txt\n",
    "\n",
    "```\n",
    "pip3 install -r requirements.txt\n",
    "```\n",
    "\n",
    "Download WordEmbeddings for encoding lexical items (Gensim KeyedVectors, or pymagnitude) into the *source/* directory.\n",
    "\n",
    "```\n",
    "cd source/\n",
    "curl -O http://magnitude.plasticity.ai/fasttext+subword/wiki-news-300d-1M.magnitude\n",
    "curl -O http://magnitude.plasticity.ai/word2vec+subword/GoogleNews-vectors-negative300.magnitude\n",
    "```\n",
    "\n",
    "- https://github.com/plasticityai/magnitude\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Download the VUAM Corpus as XML (can't be included due to copyrights) into the *starterkits/* directory. **Hint**: There is a cell in this Notebook that will do that. See [VUAM Corpus](#vuamc_generation).\n",
    "\n",
    "```\n",
    "cd starterkits/\n",
    "curl -O http://ota.ahds.ac.uk/headers/2541.xml\n",
    "\n",
    "# Or use the Python functions provided in the utils module\n",
    "python3 -i utils.py\n",
    "download_vuamc_xml()\n",
    "```\n",
    "\n",
    "The VUAMC needs to be converted into a CSV file and placed into the *source/* directory. This is done using the starterkits provided by the NAACL, which are included in the repository, or a Python function.\n",
    "\n",
    "```\n",
    "cd starterkits/\n",
    "python3 vua_xml_parser.py\n",
    "python3 vua_xml_parser_test.py\n",
    "\n",
    "# Or use the Python functions provided in the utils module\n",
    "python3 -i utils.py\n",
    "generate_vuamc_csv()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing custom modules\n",
    "import utils\n",
    "import corpus\n",
    "import evaluate\n",
    "import features\n",
    "\n",
    "# Import general dependencies\n",
    "import numpy\n",
    "import os\n",
    "import collections\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, Input, Masking, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as kerasbackend\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vuamc_generation'></a>\n",
    "# VUAM Corpus\n",
    "\n",
    "The VUAMC is the basis for this task. However, it cannot be included in the repository due to copyrights. \n",
    "\n",
    "The next Cell will check if the VUAMC is downloaded and do so if necessay. It will also generate the CSV files using the converter provided by NAACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('source/vuamc_corpus_test.csv') and not os.path.exists('source/vuamc_corpus_train.csv'):\n",
    "    print('VUAMC training and test data not found. Generating...')\n",
    "    utils.download_vuamc_xml()\n",
    "    utils.generate_vuamc_csv()\n",
    "    print('VUAMC CSV generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='corpus_generation'></a>\n",
    "# Test and Training Corpus\n",
    "\n",
    "The next cell will convert the CSV files for training and testing into a Corpus object. This is to manage the sentences in the given corpus and provide functions such as: list all labels, list all tokens, etc.\n",
    "\n",
    "The validation checks if the tokens in the corpus and the tokens in the training/test files align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and validated training corpus\n",
      "Loaded and validated test corpus\n"
     ]
    }
   ],
   "source": [
    "# Load Train Corpus from CSV\n",
    "c_train = corpus.VUAMC('source/vuamc_corpus_train.csv', 'source/verb_tokens_train_gold_labels.csv')\n",
    "c_train.validate_corpus()\n",
    "print('Loaded and validated training corpus')\n",
    "\n",
    "# Load Test Corpus from CSV\n",
    "c_test = corpus.VUAMC('source/vuamc_corpus_test.csv', 'source/verb_tokens_test.csv', mode='test')\n",
    "c_test.validate_corpus()\n",
    "print('Loaded and validated test corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='corpus_validation'></a>\n",
    "# Corpus Validation\n",
    "\n",
    "The next cell with show that the training data is highly imbalanced. For the training of the model we will use a binary classification, using 0 to encode non-metaphor tokens and 1 to encode metaphor tokens. \n",
    "\n",
    "The training set, however, includes a significantly higher amount of non-metaphor tokens. A fact that will cause simple training to fail, since due to the imbalance the model will almost always choose a 0. The calculated ratios will be used to introduce a *weighted_categorical_crossentropy* loss function to combat this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of metaphor tokens: 3%\n",
      "Percentage of non-metaphor tokens: 97%\n",
      "Ratio: 1:32\n"
     ]
    }
   ],
   "source": [
    "number_of_all_labels = len(c_train.label_list)\n",
    "count_of_label_classes = collections.Counter(c_train.label_list)\n",
    "\n",
    "percentage_of_non_metaphor_tokens = round(count_of_label_classes[0] / number_of_all_labels * 100)\n",
    "percentage_of_metaphor_tokens = round(count_of_label_classes[1] / number_of_all_labels * 100)\n",
    "ratio = utils.simplify_ratio(percentage_of_non_metaphor_tokens, percentage_of_metaphor_tokens)\n",
    "assert(percentage_of_non_metaphor_tokens + percentage_of_metaphor_tokens == 100)\n",
    "\n",
    "print('Percentage of metaphor tokens: {}%'.format(percentage_of_metaphor_tokens))\n",
    "print('Percentage of non-metaphor tokens: {}%'.format(percentage_of_non_metaphor_tokens))\n",
    "print('Ratio: {}:{}'.format(ratio[0], ratio[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_configuration'></a>\n",
    "# Model Configuration\n",
    "\n",
    "The next cell is the primary configuration for the model.\n",
    "\n",
    "## Weighted Categorical Crossentropy\n",
    "\n",
    "As described above, the training set is highly imbalanced. Therefore we will use a weighted_categorical_crossentropy to calculate the loss in the training. This loss can be adjusted here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_weights: (1, 32)\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "KFOLD_SPLIT = 5\n",
    "KERAS_OPTIMIZER = 'rmsprop'\n",
    "KERAS_METRICS = [utils.f1]\n",
    "KERAS_EPOCHS = 1\n",
    "KERAS_BATCH_SIZE = 32\n",
    "KERAS_ACTIVATION = 'softmax'\n",
    "KERAS_DROPOUT = 0.25\n",
    "\n",
    "KERAS_LOSS = utils.weighted_categorical_crossentropy(ratio)\n",
    "print('loss_weights: {}'.format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word_embeddings'></a>\n",
    "# Word Embeddings\n",
    "\n",
    "The model uses Word Embeddings to encode lexical items as real number vectors to encode semantics. \n",
    "\n",
    "The nex cell will load the Embeddings for the training and test corpus. This is done by using a polymorph Class that implements the *Embeddings* interface. This way changing embeddings is as simple as changing the Embeddings Object.\n",
    "\n",
    "After the corpora are encoded, the Embeddings object is deleted to free up some memory (some embedding models use lazy loading, which would not use up memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7873/7873 [00:00<00:00, 8477.43it/s]\n",
      "100%|██████████| 2694/2694 [00:00<00:00, 9671.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Word Embeddings\n",
      "Deleted Word Embeddings\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use different Embeddings\n",
    "# embeddings = features.Word2Vec()\n",
    "# embeddings = features.Magnitude()\n",
    "embeddings = features.DummyEmbeddings(EMBEDDING_DIM)\n",
    "\n",
    "x_input, y_labels = features.generate_input_and_labels(c_train.sentences, Vectors=embeddings)\n",
    "x_test, y_test = features.generate_input_and_labels(c_test.sentences, Vectors=embeddings)\n",
    "print('Generated Word Embeddings')\n",
    "\n",
    "# Free up some memory\n",
    "del embeddings\n",
    "print('Deleted Embeddings Object')\n",
    "\n",
    "# Training labels need to be categorical, with 2 classes (0-non-metaphor, 1-metaphor)\n",
    "y_labels = to_categorical(y_labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_compilation'></a>\n",
    "# The Model\n",
    "\n",
    "This call compiles the model used in the Task.\n",
    "\n",
    " - Input: The input layer will receive the encoded sentences. Shape: Sentence Length * Embedding Dimensions\n",
    " - Core: The core of the model is a bidirectionsal LSTM with a recurrent Dropout\n",
    " - Output: The output layer is dense time distributed series with predicions for 2 classes (0|1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 2)             402       \n",
      "=================================================================\n",
      "Total params: 321,202\n",
      "Trainable params: 321,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(MAX_SENTENCE_LENGTH, EMBEDDING_DIM))\n",
    "model = Masking(mask_value=[-1] * EMBEDDING_DIM)(inputs)\n",
    "model = Bidirectional(LSTM(100, return_sequences=True, dropout=0, recurrent_dropout=KERAS_DROPOUT))(model)\n",
    "outputs = TimeDistributed(Dense(2, activation=KERAS_ACTIVATION))(model)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=KERAS_OPTIMIZER, loss=KERAS_LOSS, metrics=KERAS_METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "# Generate Training and Validation split\n",
    "\n",
    "To futher optimize the training we will use a Kfold split on the training and validation data. This will split the input data and labels *n* times and fit the model each time on the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6298 samples, validate on 1575 samples\n",
      "Epoch 1/1\n",
      "6298/6298 [==============================] - 20s 3ms/step - loss: 0.5142 - f1: 0.8015 - val_loss: 0.5140 - val_f1: 0.7088\n",
      "1575/1575 [==============================] - 2s 1ms/step\n",
      "Test score: 51.40%\n",
      "Test accuracy: 70.88%\n",
      "Train on 6298 samples, validate on 1575 samples\n",
      "Epoch 1/1\n",
      "6298/6298 [==============================] - 21s 3ms/step - loss: 0.5133 - f1: 0.7986 - val_loss: 0.5205 - val_f1: 0.6658\n",
      "1575/1575 [==============================] - 2s 1ms/step\n",
      "Test score: 52.05%\n",
      "Test accuracy: 66.58%\n",
      "Train on 6298 samples, validate on 1575 samples\n",
      "Epoch 1/1\n",
      "6298/6298 [==============================] - 19s 3ms/step - loss: 0.5416 - f1: 0.7997 - val_loss: 0.5035 - val_f1: 0.7207\n",
      "1575/1575 [==============================] - 2s 1ms/step\n",
      "Test score: 50.35%\n",
      "Test accuracy: 72.07%\n",
      "Train on 6299 samples, validate on 1574 samples\n",
      "Epoch 1/1\n",
      "6299/6299 [==============================] - 20s 3ms/step - loss: 0.5126 - f1: 0.8031 - val_loss: 0.5035 - val_f1: 0.7225\n",
      "1574/1574 [==============================] - 2s 1ms/step\n",
      "Test score: 50.35%\n",
      "Test accuracy: 72.25%\n",
      "Train on 6299 samples, validate on 1574 samples\n",
      "Epoch 1/1\n",
      "6299/6299 [==============================] - 19s 3ms/step - loss: 0.5057 - f1: 0.8108 - val_loss: 0.5134 - val_f1: 0.7764\n",
      "1574/1574 [==============================] - 2s 1ms/step\n",
      "Test score: 51.34%\n",
      "Test accuracy: 77.64%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=KFOLD_SPLIT, shuffle=True, random_state=1337)\n",
    "histories = []\n",
    "\n",
    "for train, test in kfold.split(x_input, y_labels):\n",
    "    x_train = x_input[train]\n",
    "    x_val = x_input[test]\n",
    "    y_train = y_labels[train]\n",
    "    y_val = y_labels[test]\n",
    "\n",
    "    # Fit the model for each split\n",
    "    history = model.fit(x_train, y_train,\n",
    "                  batch_size=KERAS_BATCH_SIZE,\n",
    "                  epochs=KERAS_EPOCHS,\n",
    "                  validation_data=(x_val, y_val))\n",
    "    \n",
    "    histories.append(history)\n",
    "\n",
    "    scores = model.evaluate(x_val, y_val)\n",
    "    print('Test score: {:.2%}'.format(scores[0]))\n",
    "    print('Test accuracy: {:.2%}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "# Prediction and Evalutation\n",
    "\n",
    "To evalutate the model we will use the test corpus and generate predictions (labels) for the input sentences. Each sentence will receive a list of binary classes (0|1) for its tokens. \n",
    "\n",
    "The predictions will be saved in a CSV file, which will be simimlar to the Gold Labels from the NAACL. Using both of these files (predicitions and gold-standards) we will evalutate the perfomance of the model. \n",
    "\n",
    "Performance is measured in Precision and Recall, expressed in the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(precision=0.2785515320334262, recall=0.4851425106124924, f1=0.35390400353904)\n"
     ]
    }
   ],
   "source": [
    "# Get float predictions and turn them into binaries\n",
    "float_predictions = model.predict(x_test, batch_size=KERAS_BATCH_SIZE)\n",
    "binary_predictions = kerasbackend.argmax(float_predictions)\n",
    "label_predictions = kerasbackend.eval(binary_predictions)\n",
    "\n",
    "# Write prediction to CSV file\n",
    "predictions_file = 'predictions.csv'\n",
    "standard_file = 'source/verb_tokens_test_gold_labels.csv'\n",
    "\n",
    "# Write the predictions.csv and compare to gold standard\n",
    "rows = evaluate.corpus_evaluation(c_test, label_predictions, MAX_SENTENCE_LENGTH)\n",
    "evaluate.csv_evalutation(rows, predictions_file)\n",
    "results = evaluate.precision_recall_f1(predictions_file, standard_file)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_plot'></a>\n",
    "# Model Training Plot\n",
    "\n",
    "The following plot shows the learning of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Loss",
         "type": "scatter",
         "uid": "a9cbd646-b2d0-11e8-860b-94b86d86a98e",
         "y": [
          0.5141704864788146,
          0.5133331799060438,
          0.5415501061149081,
          0.5126415901211334,
          0.5057438438157086
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Validation Loss",
         "type": "scatter",
         "uid": "a9cbd647-b2d0-11e8-860b-94b86d86a98e",
         "y": [
          0.5140452200647384,
          0.5204609158682445,
          0.5034543553609697,
          0.5035112625156001,
          0.5134385310709855
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Categorical Accuracy",
         "type": "scatter",
         "uid": "a9cbd648-b2d0-11e8-860b-94b86d86a98e",
         "y": [
          0.8014798324310276,
          0.7985678484190756,
          0.7997109380439185,
          0.8031337325734588,
          0.8108079514913775
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Validation Categorical Accuracy",
         "type": "scatter",
         "uid": "a9cbd649-b2d0-11e8-860b-94b86d86a98e",
         "y": [
          0.7088129229015774,
          0.6658033404842255,
          0.7206855401349446,
          0.7224521904312822,
          0.7763786318190065
         ]
        }
       ],
       "layout": {
        "title": "Training History",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "title": "Value"
        }
       }
      },
      "text/html": [
       "<div id=\"9f0daeae-634f-443b-9e57-e89891a55337\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9f0daeae-634f-443b-9e57-e89891a55337\", [{\"mode\": \"lines+markers\", \"name\": \"Loss\", \"y\": [0.5141704864788146, 0.5133331799060438, 0.5415501061149081, 0.5126415901211334, 0.5057438438157086], \"type\": \"scatter\", \"uid\": \"a9cbd646-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Loss\", \"y\": [0.5140452200647384, 0.5204609158682445, 0.5034543553609697, 0.5035112625156001, 0.5134385310709855], \"type\": \"scatter\", \"uid\": \"a9cbd647-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Categorical Accuracy\", \"y\": [0.8014798324310276, 0.7985678484190756, 0.7997109380439185, 0.8031337325734588, 0.8108079514913775], \"type\": \"scatter\", \"uid\": \"a9cbd648-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Categorical Accuracy\", \"y\": [0.7088129229015774, 0.6658033404842255, 0.7206855401349446, 0.7224521904312822, 0.7763786318190065], \"type\": \"scatter\", \"uid\": \"a9cbd649-b2d0-11e8-860b-94b86d86a98e\"}], {\"title\": \"Training History\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"9f0daeae-634f-443b-9e57-e89891a55337\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"9f0daeae-634f-443b-9e57-e89891a55337\", [{\"mode\": \"lines+markers\", \"name\": \"Loss\", \"y\": [0.5141704864788146, 0.5133331799060438, 0.5415501061149081, 0.5126415901211334, 0.5057438438157086], \"type\": \"scatter\", \"uid\": \"a9cbd646-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Loss\", \"y\": [0.5140452200647384, 0.5204609158682445, 0.5034543553609697, 0.5035112625156001, 0.5134385310709855], \"type\": \"scatter\", \"uid\": \"a9cbd647-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Categorical Accuracy\", \"y\": [0.8014798324310276, 0.7985678484190756, 0.7997109380439185, 0.8031337325734588, 0.8108079514913775], \"type\": \"scatter\", \"uid\": \"a9cbd648-b2d0-11e8-860b-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Categorical Accuracy\", \"y\": [0.7088129229015774, 0.6658033404842255, 0.7206855401349446, 0.7224521904312822, 0.7763786318190065], \"type\": \"scatter\", \"uid\": \"a9cbd649-b2d0-11e8-860b-94b86d86a98e\"}], {\"title\": \"Training History\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly \n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "loss_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['loss'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Loss'\n",
    ")\n",
    "\n",
    "val_loss_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['val_loss'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Loss'\n",
    ")\n",
    "\n",
    "acc_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['f1'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Categorical Accuracy'\n",
    ")\n",
    "\n",
    "val_acc_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['val_f1'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Categorical Accuracy'\n",
    ")\n",
    "\n",
    "layout = plotly.graph_objs.Layout(title=\"Training History\",\n",
    "                yaxis=dict(title='Value'),\n",
    "                xaxis=dict(title='Epoch'))\n",
    "\n",
    "data = [loss_p, val_loss_p, acc_p, val_acc_p]\n",
    "fig = plotly.graph_objs.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig, filename='jupyter-train-history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
