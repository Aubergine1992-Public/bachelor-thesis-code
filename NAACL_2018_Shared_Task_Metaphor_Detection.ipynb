{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAACL 2018 Shared Task - Metaphor Detection\n",
    "\n",
    "This notebook implements a method for Metaphor Detection using Keras, as part of the (thesis)[TODO]. It is based on the (NAACL 2018 Shared Task for Metaphor Detection)[TODO] but did not compete in the task.\n",
    "\n",
    "For futher details on the Shared Task, visit: https://github.com/EducationalTestingService/metaphor/tree/master/NAACL-FLP-shared-task\n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "Install the Python3 requirements from the requirements.txt\n",
    "\n",
    "```\n",
    "pip3 install -r requirements.txt\n",
    "```\n",
    "\n",
    "Download WordEmbeddings for encoding lexical items (Gensim KeyedVectors, or Pymagnitude)\n",
    "\n",
    "```\n",
    "curl TODO\n",
    "```\n",
    "\n",
    "Download the VUAM Corpus (can't be included due to copyrights)\n",
    "\n",
    "```\n",
    "curl http://ota.ahds.ac.uk/headers/2541.xml\n",
    "# Or use python functions provided in utils module (see below)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom modules\n",
    "import utils\n",
    "import corpus\n",
    "import evaluate\n",
    "import features\n",
    "\n",
    "# Import general dependencies\n",
    "import numpy\n",
    "import os\n",
    "import collections\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, Input, Masking, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as kerasbackend\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VUAM Corpus\n",
    "\n",
    "The VUAMC is the basis for this task. However, it cannot be included in the repository due to copyrights. \n",
    "\n",
    "The next Cell will check if the VUAMC is downloaded and do so if necessay. It will also generate the CSV files using the converter provided by NAACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('source/vuamc_corpus_test.csv') and not os.path.exists('source/vuamc_corpus_train.csv'):\n",
    "    print('VUAMC training and test data not found. Generating...')\n",
    "    utils.download_vuamc_xml()\n",
    "    utils.generate_vuamc_csv()\n",
    "    print('VUAMC CSV generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Training Corpus\n",
    "\n",
    "The next cell will convert the CSV files for training and testing into a Corpus object. This is to manage the sentences in the given corpus and provide functions such as: list all labels, list all tokens, etc.\n",
    "\n",
    "The validation checks if the tokens in the corpus and the tokens in the training/test files align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Corpus from CSV\n",
    "c_train = corpus.VUAMC('source/vuamc_corpus_train.csv', 'source/verb_tokens_train_gold_labels.csv')\n",
    "c_train.validate_corpus()\n",
    "print('Loaded and validated training corpus')\n",
    "\n",
    "# Load Test Corpus from CSV\n",
    "c_test = corpus.VUAMC('source/vuamc_corpus_test.csv', 'source/verb_tokens_test.csv', mode='test')\n",
    "c_test.validate_corpus()\n",
    "print('Loaded and validated test corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Validation\n",
    "\n",
    "The next cell with show that the training data is highly imbalanced. For the training of the model we will use a binary classification, using 0 to encode non-metaphor tokens and 1 to encode metaphor tokens. \n",
    "\n",
    "The training set, however, includes a significantly higher amount of non-metaphor tokens. A fact that will cause simple training to fail, since due to the imbalance the model will almost always choose a 0. The calculated ratios will be used to introduce a *weighted_categorical_crossentropy* loss function to combat this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_all_labels = len(c_train.label_list)\n",
    "count_of_label_classes = collections.Counter(c_train.label_list)\n",
    "\n",
    "percentage_of_non_metaphor_tokens = round(count_of_label_classes[0] / number_of_all_labels * 100)\n",
    "percentage_of_metaphor_tokens = round(count_of_label_classes[1] / number_of_all_labels * 100)\n",
    "ratio = utils.simplify_ratio(percentage_of_non_metaphor_tokens, percentage_of_metaphor_tokens)\n",
    "assert(percentage_of_non_metaphor_tokens + percentage_of_metaphor_tokens == 100)\n",
    "\n",
    "print('Percentage of metaphor tokens: {}'.format(percentage_of_metaphor_tokens))\n",
    "print('Percentage of non-metaphor tokens: {}'.format(percentage_of_non_metaphor_tokens))\n",
    "print('Ratio: {}:{}'.format(ratio[0], ratio[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration\n",
    "\n",
    "The next cell is the primary configuration for the model.\n",
    "\n",
    "## Weighted Categorical Crossentropy\n",
    "\n",
    "As described above, the training set is highly imbalanced. Therefore we will use a weighted_categorical_crossentropy to calculate the loss in the training. This loss can be adjusted here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300\n",
    "KFOLD_SPLIT = 5\n",
    "KERAS_OPTIMIZER = 'rmsprop'\n",
    "KERAS_METRICS = [utils.f1]\n",
    "KERAS_EPOCHS = 1\n",
    "KERAS_BATCH_SIZE = 32\n",
    "KERAS_ACTIVATION = 'softmax'\n",
    "KERAS_DROPOUT = 0.25\n",
    "\n",
    "KERAS_LOSS = utils.weighted_categorical_crossentropy(ratio)\n",
    "print('loss_weights: {}'.format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "The model uses Word Embeddings to encode lexical items as real number vectors to encode semantics. \n",
    "\n",
    "The nex cell will load the Embeddings for the training and test corpus. This is done by using a polymorph Class that implements the *Embeddings* interface. This way changing embeddings is as simple as changing the Embeddings Object.\n",
    "\n",
    "After the corpora are encoded, the Embeddings object is deleted to free up some memory (some embedding models use lazy loading, which would not use up memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use different Embeddings\n",
    "# embeddings = features.Word2Vec()\n",
    "# embeddings = features.Magnitude()\n",
    "embeddings = features.DummyEmbeddings(EMBEDDING_DIM)\n",
    "\n",
    "x_input, y_labels = features.generate_input_and_labels(c_train.sentences, Vectors=embeddings)\n",
    "x_test, y_test = features.generate_input_and_labels(c_test.sentences, Vectors=embeddings)\n",
    "\n",
    "# Free up some memory\n",
    "del embeddings\n",
    "print('Deleted Word Embeddings')\n",
    "\n",
    "# Training labels need to be categorical, with 2 classes (0-non-metaphor, 1-metaphor)\n",
    "y_labels = to_categorical(y_labels, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "This call compiles the model used in the Task.\n",
    "\n",
    " - Input: The input layer will receive the encoded sentences. Shape: Sentence Length * Embedding Dimensions\n",
    " - Core: The core of the model is a bidirectionsal LSTM with a recurrent Dropout\n",
    " - Output: The output layer is dense time distributed series with predicions for 2 classes (0|1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(MAX_SENTENCE_LENGTH, EMBEDDING_DIM))\n",
    "model = Masking(mask_value=[-1] * EMBEDDING_DIM)(inputs)\n",
    "model = Bidirectional(LSTM(100, return_sequences=True, dropout=0, recurrent_dropout=KERAS_DROPOUT))(model)\n",
    "outputs = TimeDistributed(Dense(2, activation=KERAS_ACTIVATION))(model)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=KERAS_OPTIMIZER, loss=KERAS_LOSS, metrics=KERAS_METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training and Validation split\n",
    "\n",
    "To futher optimize the training we will use a Kfold split on the training and validation data. This will split the input data and labels *n* times and fit the model each time on the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=KFOLD_SPLIT, shuffle=True, random_state=1337)\n",
    "history = []\n",
    "\n",
    "for train, test in kfold.split(x_input, y_labels):\n",
    "    x_train = x_input[train]\n",
    "    x_val = x_input[test]\n",
    "    y_train = y_labels[train]\n",
    "    y_val = y_labels[test]\n",
    "\n",
    "    # Fit the model for each split\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=KERAS_BATCH_SIZE,\n",
    "              epochs=KERAS_EPOCHS,\n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "    # TODO: save history for all epochs\n",
    "\n",
    "    scores = model.evaluate(x_val, y_val)\n",
    "    print('Test score: {:.2%}'.format(scores[0]))\n",
    "    print('Test accuracy: {:.2%}'.format(scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Evalutation\n",
    "\n",
    "To evalutate the model we will use the test corpus and generate predictions (labels) for the input sentences. Each sentence will receive a list of binary classes (0|1) for its tokens. \n",
    "\n",
    "The predictions will be saved in a CSV file, which will be simimlar to the Gold Labels from the NAACL. Using both of these files (predicitions and gold-standards) we will evalutate the perfomance of the model. \n",
    "\n",
    "Performance is measured in Precision and Recall, expressed in the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get float predictions and turn them into binaries\n",
    "float_predictions = model.predict(x_test, batch_size=KERAS_BATCH_SIZE)\n",
    "binary_predictions = kerasbackend.argmax(float_predictions)\n",
    "label_predictions = kerasbackend.eval(binary_predictions)\n",
    "\n",
    "# Write prediction to CSV file\n",
    "predictions_file = 'predictions.csv'\n",
    "standard_file = 'source/verb_tokens_test_gold_labels.csv'\n",
    "\n",
    "# Write the predictions.csv and compare to gold standard\n",
    "rows = evaluate.corpus_evaluation(c_test, label_predictions, MAX_SENTENCE_LENGTH)\n",
    "evaluate.csv_evalutation(rows, predictions_file)\n",
    "results = evaluate.precision_recall_f1(predictions_file, standard_file)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Plot\n",
    "\n",
    "The following plot shows the learning of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly \n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "loss_p = plotly.graph_objs.Scatter(\n",
    "    y = model.history.history['loss'],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Loss'\n",
    ")\n",
    "\n",
    "val_loss_p = plotly.graph_objs.Scatter(\n",
    "    y = model.history.history['val_loss'],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Loss'\n",
    ")\n",
    "\n",
    "acc_p = plotly.graph_objs.Scatter(\n",
    "    y = model.history.history['f1'],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Categorical Accuracy'\n",
    ")\n",
    "\n",
    "val_acc_p = plotly.graph_objs.Scatter(\n",
    "    y = model.history.history['val_f1'],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Categorical Accuracy'\n",
    ")\n",
    "\n",
    "layout = plotly.graph_objs.Layout(title=\"Training History\",\n",
    "                yaxis=dict(title='Value'),\n",
    "                xaxis=dict(title='Epoch'))\n",
    "\n",
    "data = [loss_p, acc_p, val_loss_p, val_acc_p]\n",
    "fig = plotly.graph_objs.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig, filename='jupyter-train-history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
