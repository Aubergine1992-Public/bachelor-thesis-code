{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAACL 2018 Shared Task - Metaphor Detection\n",
    "\n",
    "This notebook implements a method for Metaphor Detection using Keras, as part of my [bachelor thesis](TODO). It is based on the [NAACL 2018 Shared Task for Metaphor Detection](https://sites.google.com/site/figlangworkshop/shared-task) but did not compete in the task.\n",
    "\n",
    "For further details on the Shared Task dataset, visit: https://github.com/EducationalTestingService/metaphor/tree/master/NAACL-FLP-shared-task\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Prerequisites](#prerequisites)\n",
    "- [Download VUAM Corpus](#vuamc_generation)\n",
    "- [Generate Training and Test Data](#corpus_generation)\n",
    "- [Validate Training and Test Data](#corpus_validation)\n",
    "- [Keras Model Configuration](#model_configuration)\n",
    "- [Load Word Embeddings](#word_embeddings)\n",
    "- [Keras Model Compilation](#model_compilation)\n",
    "- [Model Training](#training)\n",
    "- [Model Evaluation](#evaluation)\n",
    "- [Plot of Training](#training_plot)\n",
    "\n",
    "<a id='prerequisites'></a>\n",
    "## Prerequisites \n",
    "\n",
    "Install the Python3 requirements from the requirements.txt\n",
    "\n",
    "```\n",
    "pip3 install -r requirements.txt\n",
    "```\n",
    "\n",
    "Download WordEmbeddings for encoding lexical items (Gensim KeyedVectors, or pymagnitude) into the *source/* directory.\n",
    "\n",
    "```\n",
    "cd source/\n",
    "curl -O http://magnitude.plasticity.ai/fasttext+subword/wiki-news-300d-1M.magnitude\n",
    "curl -O http://magnitude.plasticity.ai/word2vec+subword/GoogleNews-vectors-negative300.magnitude\n",
    "```\n",
    "\n",
    "- https://github.com/plasticityai/magnitude\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Download the VUAM Corpus as XML (can't be included due to copyrights) into the *starterkits/* directory. **Hint**: There is a cell in this Notebook that will do that. See [VUAM Corpus](#vuamc_generation).\n",
    "\n",
    "```\n",
    "cd starterkits/\n",
    "curl -O http://ota.ahds.ac.uk/headers/2541.xml\n",
    "\n",
    "# Or use the Python functions provided in the utils module\n",
    "python3 -i utils.py\n",
    "download_vuamc_xml()\n",
    "```\n",
    "\n",
    "The VUAMC needs to be converted into a CSV file and placed into the *source/* directory. This is done using the starterkits provided by the NAACL, which are included in the repository, or a Python function.\n",
    "\n",
    "```\n",
    "cd starterkits/\n",
    "python3 vua_xml_parser.py\n",
    "python3 vua_xml_parser_test.py\n",
    "\n",
    "# Or use the Python functions provided in the utils module\n",
    "python3 -i utils.py\n",
    "generate_vuamc_csv()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing custom modules\n",
    "import utils\n",
    "import corpus\n",
    "import evaluate\n",
    "import features\n",
    "\n",
    "# Import general dependencies\n",
    "import numpy\n",
    "import os\n",
    "import collections\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Bidirectional, LSTM, Input, Masking, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as kerasbackend\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vuamc_generation'></a>\n",
    "# VUAM Corpus\n",
    "\n",
    "The VUAMC is the basis for this task. However, it cannot be included in the repository due to copyrights. \n",
    "\n",
    "The next Cell will check if the VUAMC is downloaded and do so if necessay. It will also generate the CSV files using the converter provided by NAACL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('source/vuamc_corpus_test.csv') and not os.path.exists('source/vuamc_corpus_train.csv'):\n",
    "    print('VUAMC training and test data not found. Generating...')\n",
    "    utils.download_vuamc_xml()\n",
    "    utils.generate_vuamc_csv()\n",
    "    print('VUAMC CSV generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='corpus_generation'></a>\n",
    "# Test and Training Corpus\n",
    "\n",
    "The next cell will convert the CSV files for training and testing into a Corpus object. This is to manage the sentences in the given corpus and provide functions such as: list all labels, list all tokens, etc.\n",
    "\n",
    "The validation checks if the tokens in the corpus and the tokens in the training/test files align."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and validated training corpus\n",
      "Loaded and validated test corpus\n"
     ]
    }
   ],
   "source": [
    "# Load Train Corpus from CSV\n",
    "#c_train = corpus.VUAMC('source/vuamc_corpus_train.csv', 'source/verb_tokens_train_gold_labels.csv', 'source/vuamc_corpus_train_pos.csv')\n",
    "c_train = corpus.VUAMC('source/vuamc_corpus_train.csv', 'source/all_pos_tokens_train_gold_labels.csv', 'source/vuamc_corpus_train_pos.csv')\n",
    "c_train.validate_corpus()\n",
    "print('Loaded and validated training corpus')\n",
    "\n",
    "# Load Test Corpus from CSV\n",
    "#c_test = corpus.VUAMC('source/vuamc_corpus_test.csv', 'source/verb_tokens_test.csv', 'source/vuamc_corpus_test_pos.csv', mode='test')\n",
    "c_test = corpus.VUAMC('source/vuamc_corpus_test.csv', 'source/all_pos_tokens_test.csv', 'source/vuamc_corpus_test_pos.csv', mode='test')\n",
    "c_test.validate_corpus()\n",
    "print('Loaded and validated test corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='corpus_validation'></a>\n",
    "# Corpus Validation\n",
    "\n",
    "The next cell with show that the training data is highly imbalanced. For the training of the model we will use a binary classification, using 0 to encode non-metaphor tokens and 1 to encode metaphor tokens. \n",
    "\n",
    "The training set, however, includes a significantly higher amount of non-metaphor tokens. A fact that will cause simple training to fail, since due to the imbalance the model will almost always choose a 0. The calculated ratios will be used to introduce a *weighted_categorical_crossentropy* loss function to combat this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of metaphor tokens: 6%\n",
      "Percentage of non-metaphor tokens: 94%\n",
      "Ratio: 1:16\n"
     ]
    }
   ],
   "source": [
    "number_of_all_labels = len(c_train.label_list)\n",
    "count_of_label_classes = collections.Counter(c_train.label_list)\n",
    "\n",
    "percentage_of_non_metaphor_tokens = round(count_of_label_classes[0] / number_of_all_labels * 100)\n",
    "percentage_of_metaphor_tokens = round(count_of_label_classes[1] / number_of_all_labels * 100)\n",
    "ratio = utils.simplify_ratio(percentage_of_non_metaphor_tokens, percentage_of_metaphor_tokens)\n",
    "assert(percentage_of_non_metaphor_tokens + percentage_of_metaphor_tokens == 100)\n",
    "\n",
    "print('Percentage of metaphor tokens: {}%'.format(percentage_of_metaphor_tokens))\n",
    "print('Percentage of non-metaphor tokens: {}%'.format(percentage_of_non_metaphor_tokens))\n",
    "print('Ratio: {}:{}'.format(ratio[0], ratio[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_configuration'></a>\n",
    "# Model Configuration\n",
    "\n",
    "The next cell is the primary configuration for the model.\n",
    "\n",
    "## Weighted Categorical Crossentropy\n",
    "\n",
    "As described above, the training set is highly imbalanced. Therefore we will use a weighted_categorical_crossentropy to calculate the loss in the training. This loss can be adjusted here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_weight [1.0, 15.119793553060486]\n"
     ]
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 50\n",
    "WEIGHT_SMOOTHING = 0.0\n",
    "EMBEDDING_DIM = 300\n",
    "KFOLD_SPLIT = 8\n",
    "KERAS_OPTIMIZER = 'rmsprop'\n",
    "KERAS_METRICS = [utils.precision, utils.recall, utils.f1]\n",
    "KERAS_EPOCHS = 5\n",
    "KERAS_BATCH_SIZE = 32\n",
    "KERAS_ACTIVATION = 'softmax'\n",
    "KERAS_DROPOUT = 0.25\n",
    "\n",
    "class_weights =  list(utils.get_class_weights(c_train.label_list, WEIGHT_SMOOTHING).values())\n",
    "print('loss_weight {}'.format(class_weights))\n",
    "KERAS_LOSS = utils.weighted_categorical_crossentropy(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word_embeddings'></a>\n",
    "# Word Embeddings\n",
    "\n",
    "The model uses Word Embeddings to encode lexical items as real number vectors to encode semantics. \n",
    "\n",
    "The nex cell will load the Embeddings for the training and test corpus. This is done by using a polymorph Class that implements the *Embeddings* interface. This way changing embeddings is as simple as changing the Embeddings Object.\n",
    "\n",
    "After the corpora are encoded, the Embeddings object is deleted to free up some memory (some embedding models use lazy loading, which would not use up memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11182/11182 [00:01<00:00, 8563.92it/s]\n",
      "100%|██████████| 3794/3794 [00:00<00:00, 11598.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Word Embeddings\n",
      "Deleted Embeddings Object\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to use different Embeddings\n",
    "# embeddings = features.Word2Vec()\n",
    "# embeddings = features.Magnitude()\n",
    "embeddings = features.DummyEmbeddings(EMBEDDING_DIM)\n",
    "\n",
    "x_input, y_labels, z_postags = features.generate_input_and_labels(c_train.sentences, Vectors=embeddings)\n",
    "x_test, y_test, z_testtags = features.generate_input_and_labels(c_test.sentences, Vectors=embeddings)\n",
    "print('Generated Word Embeddings')\n",
    "\n",
    "# Free up some memory\n",
    "del embeddings\n",
    "print('Deleted Embeddings Object')\n",
    "\n",
    "# POS Tags to numerical sequences\n",
    "pos_tokenizer = Tokenizer()\n",
    "pos_tokenizer.fit_on_texts(z_postags)\n",
    "pos_sequences = pos_tokenizer.texts_to_sequences(z_postags)\n",
    "pos_test_sequences = pos_tokenizer.texts_to_sequences(z_testtags)\n",
    "\n",
    "# Training labels need to be categorical, with 2 classes (0-non-metaphor, 1-metaphor)\n",
    "y_labels = to_categorical(y_labels, 2)\n",
    "z_pos = to_categorical(pos_sequences)\n",
    "z_test = to_categorical(pos_test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model_compilation'></a>\n",
    "# The Model\n",
    "\n",
    "This call compiles the model used in the Task.\n",
    "\n",
    " - Input: The input layer will receive the encoded sentences. Shape: Sentence Length * Embedding Dimensions\n",
    " - Core: The core of the model is a bidirectionsal LSTM with a recurrent Dropout\n",
    " - Output: The output layer is dense time distributed series with predicions for 2 classes (0|1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 50, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 200)           320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 2)             402       \n",
      "=================================================================\n",
      "Total params: 321,202\n",
      "Trainable params: 321,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "postags = Input(shape=(MAX_SENTENCE_LENGTH, 17))\n",
    "sentences = Input(shape=(MAX_SENTENCE_LENGTH, EMBEDDING_DIM))\n",
    "model = Masking(mask_value=[-1] * EMBEDDING_DIM)(sentences)\n",
    "model = Bidirectional(LSTM(100, return_sequences=True, dropout=0, recurrent_dropout=KERAS_DROPOUT))(model)\n",
    "outputs = TimeDistributed(Dense(2, activation=KERAS_ACTIVATION))(model)\n",
    "model = Model(inputs=sentences, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=KERAS_OPTIMIZER, loss=KERAS_LOSS, metrics=KERAS_METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "# Generate Training and Validation split\n",
    "\n",
    "To futher optimize the training we will use a Kfold split on the training and validation data. This will split the input data and labels *n* times and fit the model each time on the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.4859 - precision: 0.8330 - recall: 0.8330 - f1: 0.8330 - val_loss: 0.3888 - val_precision: 0.7641 - val_recall: 0.7641 - val_f1: 0.7641\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 26s 3ms/step - loss: 0.4033 - precision: 0.8259 - recall: 0.8259 - f1: 0.8259 - val_loss: 0.4024 - val_precision: 0.7434 - val_recall: 0.7434 - val_f1: 0.7434\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.4018 - precision: 0.8189 - recall: 0.8189 - f1: 0.8189 - val_loss: 0.3902 - val_precision: 0.7609 - val_recall: 0.7609 - val_f1: 0.7609\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.4021 - precision: 0.8153 - recall: 0.8153 - f1: 0.8153 - val_loss: 0.3885 - val_precision: 0.8802 - val_recall: 0.8802 - val_f1: 0.8802\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.4000 - precision: 0.8180 - recall: 0.8180 - f1: 0.8180 - val_loss: 0.3921 - val_precision: 0.9368 - val_recall: 0.9368 - val_f1: 0.9368\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 39.21%\n",
      "Precision: 93.68%\n",
      "Recall: 93.68%\n",
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3969 - precision: 0.8218 - recall: 0.8218 - f1: 0.8218 - val_loss: 0.4131 - val_precision: 0.9656 - val_recall: 0.9656 - val_f1: 0.9656\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3960 - precision: 0.8238 - recall: 0.8238 - f1: 0.8238 - val_loss: 0.4022 - val_precision: 0.8262 - val_recall: 0.8262 - val_f1: 0.8262\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3944 - precision: 0.8266 - recall: 0.8266 - f1: 0.8266 - val_loss: 0.4075 - val_precision: 0.8825 - val_recall: 0.8825 - val_f1: 0.8825\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3932 - precision: 0.8332 - recall: 0.8332 - f1: 0.8332 - val_loss: 0.4035 - val_precision: 0.7803 - val_recall: 0.7803 - val_f1: 0.7803\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3920 - precision: 0.8352 - recall: 0.8352 - f1: 0.8352 - val_loss: 0.4053 - val_precision: 0.8592 - val_recall: 0.8592 - val_f1: 0.8592\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 40.53%\n",
      "Precision: 85.92%\n",
      "Recall: 85.92%\n",
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3918 - precision: 0.8363 - recall: 0.8363 - f1: 0.8363 - val_loss: 0.3900 - val_precision: 0.8260 - val_recall: 0.8260 - val_f1: 0.8260\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3904 - precision: 0.8377 - recall: 0.8377 - f1: 0.8377 - val_loss: 0.3923 - val_precision: 0.8930 - val_recall: 0.8930 - val_f1: 0.8930\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3891 - precision: 0.8411 - recall: 0.8411 - f1: 0.8411 - val_loss: 0.3908 - val_precision: 0.8548 - val_recall: 0.8548 - val_f1: 0.8548\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3868 - precision: 0.8453 - recall: 0.8453 - f1: 0.8453 - val_loss: 0.3967 - val_precision: 0.8079 - val_recall: 0.8079 - val_f1: 0.8079\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3849 - precision: 0.8482 - recall: 0.8482 - f1: 0.8482 - val_loss: 0.3934 - val_precision: 0.8481 - val_recall: 0.8481 - val_f1: 0.8481\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 39.34%\n",
      "Precision: 84.81%\n",
      "Recall: 84.81%\n",
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 28s 3ms/step - loss: 0.3846 - precision: 0.8485 - recall: 0.8485 - f1: 0.8485 - val_loss: 0.3794 - val_precision: 0.9036 - val_recall: 0.9036 - val_f1: 0.9036\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 28s 3ms/step - loss: 0.3816 - precision: 0.8522 - recall: 0.8522 - f1: 0.8522 - val_loss: 0.3839 - val_precision: 0.9053 - val_recall: 0.9053 - val_f1: 0.9053\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 28s 3ms/step - loss: 0.3792 - precision: 0.8548 - recall: 0.8548 - f1: 0.8548 - val_loss: 0.3882 - val_precision: 0.7860 - val_recall: 0.7860 - val_f1: 0.7860\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 28s 3ms/step - loss: 0.3765 - precision: 0.8604 - recall: 0.8604 - f1: 0.8604 - val_loss: 0.3921 - val_precision: 0.7786 - val_recall: 0.7786 - val_f1: 0.7786\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3725 - precision: 0.8627 - recall: 0.8627 - f1: 0.8627 - val_loss: 0.3828 - val_precision: 0.8333 - val_recall: 0.8333 - val_f1: 0.8333\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 38.28%\n",
      "Precision: 83.33%\n",
      "Recall: 83.33%\n",
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3736 - precision: 0.8613 - recall: 0.8613 - f1: 0.8613 - val_loss: 0.3585 - val_precision: 0.8265 - val_recall: 0.8265 - val_f1: 0.8265\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3692 - precision: 0.8647 - recall: 0.8647 - f1: 0.8647 - val_loss: 0.3538 - val_precision: 0.8578 - val_recall: 0.8578 - val_f1: 0.8578\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3653 - precision: 0.8694 - recall: 0.8694 - f1: 0.8694 - val_loss: 0.3595 - val_precision: 0.9142 - val_recall: 0.9142 - val_f1: 0.9142\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3615 - precision: 0.8734 - recall: 0.8734 - f1: 0.8734 - val_loss: 0.3566 - val_precision: 0.8673 - val_recall: 0.8673 - val_f1: 0.8673\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3561 - precision: 0.8772 - recall: 0.8772 - f1: 0.8772 - val_loss: 0.3611 - val_precision: 0.8998 - val_recall: 0.8998 - val_f1: 0.8998\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 36.11%\n",
      "Precision: 89.98%\n",
      "Recall: 89.98%\n",
      "Train on 9784 samples, validate on 1398 samples\n",
      "Epoch 1/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3543 - precision: 0.8793 - recall: 0.8793 - f1: 0.8793 - val_loss: 0.3513 - val_precision: 0.8540 - val_recall: 0.8540 - val_f1: 0.8540\n",
      "Epoch 2/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3465 - precision: 0.8840 - recall: 0.8840 - f1: 0.8840 - val_loss: 0.3473 - val_precision: 0.8793 - val_recall: 0.8793 - val_f1: 0.8793\n",
      "Epoch 3/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3434 - precision: 0.8872 - recall: 0.8872 - f1: 0.8872 - val_loss: 0.3560 - val_precision: 0.8479 - val_recall: 0.8479 - val_f1: 0.8479\n",
      "Epoch 4/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3370 - precision: 0.8903 - recall: 0.8903 - f1: 0.8903 - val_loss: 0.3550 - val_precision: 0.8822 - val_recall: 0.8822 - val_f1: 0.8822\n",
      "Epoch 5/5\n",
      "9784/9784 [==============================] - 27s 3ms/step - loss: 0.3322 - precision: 0.8925 - recall: 0.8925 - f1: 0.8925 - val_loss: 0.3577 - val_precision: 0.8847 - val_recall: 0.8847 - val_f1: 0.8847\n",
      "1398/1398 [==============================] - 2s 1ms/step\n",
      "Loss: 35.77%\n",
      "Precision: 88.47%\n",
      "Recall: 88.47%\n",
      "Train on 9785 samples, validate on 1397 samples\n",
      "Epoch 1/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3326 - precision: 0.8925 - recall: 0.8925 - f1: 0.8925 - val_loss: 0.3076 - val_precision: 0.8996 - val_recall: 0.8996 - val_f1: 0.8996\n",
      "Epoch 2/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3268 - precision: 0.8963 - recall: 0.8963 - f1: 0.8963 - val_loss: 0.3190 - val_precision: 0.8841 - val_recall: 0.8841 - val_f1: 0.8841\n",
      "Epoch 3/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3209 - precision: 0.8988 - recall: 0.8988 - f1: 0.8988 - val_loss: 0.3216 - val_precision: 0.8663 - val_recall: 0.8663 - val_f1: 0.8663\n",
      "Epoch 4/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3142 - precision: 0.9026 - recall: 0.9026 - f1: 0.9026 - val_loss: 0.3261 - val_precision: 0.8785 - val_recall: 0.8785 - val_f1: 0.8785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3087 - precision: 0.9046 - recall: 0.9046 - f1: 0.9046 - val_loss: 0.3217 - val_precision: 0.9068 - val_recall: 0.9068 - val_f1: 0.9068\n",
      "1397/1397 [==============================] - 2s 1ms/step\n",
      "Loss: 32.17%\n",
      "Precision: 90.68%\n",
      "Recall: 90.68%\n",
      "Train on 9785 samples, validate on 1397 samples\n",
      "Epoch 1/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3116 - precision: 0.9038 - recall: 0.9038 - f1: 0.9038 - val_loss: 0.2820 - val_precision: 0.8884 - val_recall: 0.8884 - val_f1: 0.8884\n",
      "Epoch 2/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.3029 - precision: 0.9075 - recall: 0.9075 - f1: 0.9075 - val_loss: 0.2814 - val_precision: 0.8934 - val_recall: 0.8934 - val_f1: 0.8934\n",
      "Epoch 3/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.2972 - precision: 0.9104 - recall: 0.9104 - f1: 0.9104 - val_loss: 0.2795 - val_precision: 0.9220 - val_recall: 0.9220 - val_f1: 0.9220\n",
      "Epoch 4/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.2911 - precision: 0.9132 - recall: 0.9132 - f1: 0.9132 - val_loss: 0.2872 - val_precision: 0.9319 - val_recall: 0.9319 - val_f1: 0.9319\n",
      "Epoch 5/5\n",
      "9785/9785 [==============================] - 27s 3ms/step - loss: 0.2871 - precision: 0.9148 - recall: 0.9148 - f1: 0.9148 - val_loss: 0.2947 - val_precision: 0.8866 - val_recall: 0.8866 - val_f1: 0.8866\n",
      "1397/1397 [==============================] - 2s 1ms/step\n",
      "Loss: 29.47%\n",
      "Precision: 88.66%\n",
      "Recall: 88.66%\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=KFOLD_SPLIT, shuffle=True, random_state=1337)\n",
    "histories = []\n",
    "\n",
    "for train, test in kfold.split(x_input, y_labels):\n",
    "    x_train = x_input[train]\n",
    "    x_val = x_input[test]\n",
    "    y_train = y_labels[train]\n",
    "    y_val = y_labels[test]\n",
    "    pos_val = z_pos[test]\n",
    "    pos_train = z_pos[train]\n",
    "\n",
    "    # Fit the model for each split\n",
    "    history = model.fit(x_train, y_train,\n",
    "                  batch_size=KERAS_BATCH_SIZE,\n",
    "                  epochs=KERAS_EPOCHS,\n",
    "                  validation_data=(x_val, y_val))\n",
    "    \n",
    "    histories.append(history)\n",
    "\n",
    "    scores = model.evaluate(x_val, y_val)\n",
    "    print('Loss: {:.2%}'.format(scores[0]))\n",
    "    print('Precision: {:.2%}'.format(scores[1]))\n",
    "    print('Recall: {:.2%}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "# Prediction and Evalutation\n",
    "\n",
    "To evalutate the model we will use the test corpus and generate predictions (labels) for the input sentences. Each sentence will receive a list of binary classes (0|1) for its tokens. \n",
    "\n",
    "The predictions will be saved in a CSV file, which will be similar to the Gold Labels from the NAACL. Using both of these files (predicitions and gold-standards) we will evalutate the perfomance of the model. \n",
    "\n",
    "Performance is measured in Precision and Recall, expressed in the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(precision=0.18300184539676032, recall=0.4732237539766702, f1=0.2639361230223274)\n"
     ]
    }
   ],
   "source": [
    "# Get float predictions and turn them into binaries\n",
    "float_predictions = model.predict(x_test, batch_size=KERAS_BATCH_SIZE)\n",
    "\n",
    "binary_predictions = kerasbackend.argmax(float_predictions)\n",
    "label_predictions = kerasbackend.eval(binary_predictions)\n",
    "\n",
    "# Write prediction to CSV file\n",
    "predictions_file = 'predictions.csv'\n",
    "# standard_file = 'source/verb_tokens_test_gold_labels.csv'\n",
    "standard_file = 'source/all_pos_tokens_test_gold_labels.csv'\n",
    "\n",
    "# Write the predictions.csv and compare to gold standard\n",
    "rows = evaluate.corpus_evaluation(c_test, label_predictions, MAX_SENTENCE_LENGTH)\n",
    "evaluate.csv_evalutation(rows, predictions_file)\n",
    "results = evaluate.precision_recall_f1(predictions_file, standard_file)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_plot'></a>\n",
    "# Model Training Plot\n",
    "\n",
    "The following plot shows the learning of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines+markers",
         "name": "Loss",
         "type": "scatter",
         "uid": "5b0c2912-bda6-11e8-ac21-94b86d86a98e",
         "y": [
          0.48585838464470255,
          0.3968717653733828,
          0.3918225909488082,
          0.38459449923633066,
          0.37356233453029314,
          0.35430954402816756,
          0.33257251111719155,
          0.3116037122101947
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Validation Loss",
         "type": "scatter",
         "uid": "5b0c2913-bda6-11e8-ac21-94b86d86a98e",
         "y": [
          0.38884332660953375,
          0.4130720012689353,
          0.39003777162882053,
          0.37940244212512125,
          0.3585155447665202,
          0.3513401973571559,
          0.3075518313270001,
          0.282040252332271
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Categorical Accuracy",
         "type": "scatter",
         "uid": "5b0c2914-bda6-11e8-ac21-94b86d86a98e",
         "y": [
          0.8329965838231908,
          0.8217722526945643,
          0.8363286630598522,
          0.848524008192981,
          0.8613204784015116,
          0.879276340502944,
          0.8924782952424148,
          0.9037997589406154
         ]
        },
        {
         "mode": "lines+markers",
         "name": "Validation Categorical Accuracy",
         "type": "scatter",
         "uid": "5b0c2915-bda6-11e8-ac21-94b86d86a98e",
         "y": [
          0.7640772098635399,
          0.9656224839165487,
          0.8259798479353069,
          0.9036194425454638,
          0.8265378846972115,
          0.8539915511195411,
          0.8995993327650754,
          0.888432553817992
         ]
        }
       ],
       "layout": {
        "title": "Training History",
        "xaxis": {
         "title": "Epoch"
        },
        "yaxis": {
         "title": "Value"
        }
       }
      },
      "text/html": [
       "<div id=\"28ab4ec8-4af5-420b-911a-fe8e614762ba\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"28ab4ec8-4af5-420b-911a-fe8e614762ba\", [{\"mode\": \"lines+markers\", \"name\": \"Loss\", \"y\": [0.48585838464470255, 0.3968717653733828, 0.3918225909488082, 0.38459449923633066, 0.37356233453029314, 0.35430954402816756, 0.33257251111719155, 0.3116037122101947], \"type\": \"scatter\", \"uid\": \"5b0c2912-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Loss\", \"y\": [0.38884332660953375, 0.4130720012689353, 0.39003777162882053, 0.37940244212512125, 0.3585155447665202, 0.3513401973571559, 0.3075518313270001, 0.282040252332271], \"type\": \"scatter\", \"uid\": \"5b0c2913-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Categorical Accuracy\", \"y\": [0.8329965838231908, 0.8217722526945643, 0.8363286630598522, 0.848524008192981, 0.8613204784015116, 0.879276340502944, 0.8924782952424148, 0.9037997589406154], \"type\": \"scatter\", \"uid\": \"5b0c2914-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Categorical Accuracy\", \"y\": [0.7640772098635399, 0.9656224839165487, 0.8259798479353069, 0.9036194425454638, 0.8265378846972115, 0.8539915511195411, 0.8995993327650754, 0.888432553817992], \"type\": \"scatter\", \"uid\": \"5b0c2915-bda6-11e8-ac21-94b86d86a98e\"}], {\"title\": \"Training History\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"28ab4ec8-4af5-420b-911a-fe8e614762ba\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"28ab4ec8-4af5-420b-911a-fe8e614762ba\", [{\"mode\": \"lines+markers\", \"name\": \"Loss\", \"y\": [0.48585838464470255, 0.3968717653733828, 0.3918225909488082, 0.38459449923633066, 0.37356233453029314, 0.35430954402816756, 0.33257251111719155, 0.3116037122101947], \"type\": \"scatter\", \"uid\": \"5b0c2912-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Loss\", \"y\": [0.38884332660953375, 0.4130720012689353, 0.39003777162882053, 0.37940244212512125, 0.3585155447665202, 0.3513401973571559, 0.3075518313270001, 0.282040252332271], \"type\": \"scatter\", \"uid\": \"5b0c2913-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Categorical Accuracy\", \"y\": [0.8329965838231908, 0.8217722526945643, 0.8363286630598522, 0.848524008192981, 0.8613204784015116, 0.879276340502944, 0.8924782952424148, 0.9037997589406154], \"type\": \"scatter\", \"uid\": \"5b0c2914-bda6-11e8-ac21-94b86d86a98e\"}, {\"mode\": \"lines+markers\", \"name\": \"Validation Categorical Accuracy\", \"y\": [0.7640772098635399, 0.9656224839165487, 0.8259798479353069, 0.9036194425454638, 0.8265378846972115, 0.8539915511195411, 0.8995993327650754, 0.888432553817992], \"type\": \"scatter\", \"uid\": \"5b0c2915-bda6-11e8-ac21-94b86d86a98e\"}], {\"title\": \"Training History\", \"xaxis\": {\"title\": \"Epoch\"}, \"yaxis\": {\"title\": \"Value\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly \n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "loss_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['loss'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Loss'\n",
    ")\n",
    "\n",
    "val_loss_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['val_loss'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Loss'\n",
    ")\n",
    "\n",
    "acc_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['f1'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Categorical Accuracy'\n",
    ")\n",
    "\n",
    "val_acc_p = plotly.graph_objs.Scatter(\n",
    "    y = [history.history['val_f1'][0] for history in histories],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Validation Categorical Accuracy'\n",
    ")\n",
    "\n",
    "layout = plotly.graph_objs.Layout(title=\"Training History\",\n",
    "                yaxis=dict(title='Value'),\n",
    "                xaxis=dict(title='Epoch'))\n",
    "\n",
    "data = [loss_p, val_loss_p, acc_p, val_acc_p]\n",
    "fig = plotly.graph_objs.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig, filename='jupyter-train-history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
